{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i79ByCbCSNJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time, asyncio, aiohttp, nest_asyncio, os\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cria o cliente da API e variáveis principais\n",
        "client = OpenAI(api_key=\"chave\")\n",
        "csv_path = \"/content/drive/MyDrive/Portugues/test_dataset.csv\"\n",
        "output_path = \"/content/resultados_llm.csv\"\n",
        "modelo = \"gpt-4o-mini\"\n",
        "\n",
        "# Lê o dataset e exibe quantas notícias tem\n",
        "df = pd.read_csv(csv_path, sep=\";\", encoding=\"utf-8\")\n",
        "print(f\"Total de notícias: {len(df)}\")\n",
        "\n",
        "# envia cada texto p gpt\n",
        "#retorna 1 ou 0\n",
        "async def classificar(session, texto):\n",
        "    prompt = f\"\"\"\n",
        "    Classifique o seguinte texto de notícia como REAL ou FAKE.\n",
        "    Responda com uma palavra: REAL ou FAKE.\n",
        "    Texto: \"{texto}\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        async with session.post(\n",
        "            \"https://api.openai.com/v1/chat/completions\",\n",
        "            headers={\"Authorization\": f\"Bearer {client.api_key}\"},\n",
        "            json={\n",
        "                \"model\": modelo,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"temperature\": 0  # evita respostas aleatórias\n",
        "            },\n",
        "            timeout=60\n",
        "        ) as resp:\n",
        "            data = await resp.json()\n",
        "            resposta = data[\"choices\"][0][\"message\"][\"content\"].strip().upper()\n",
        "            if resposta not in [\"REAL\", \"FAKE\"]:\n",
        "                resposta = \"FAKE\"\n",
        "            return 0 if resposta == \"FAKE\" else 1\n",
        "    except Exception as e:\n",
        "        print(\"Erro:\", e)\n",
        "        return np.nan  # retorna nulo se algo falhar\n",
        "\n",
        "# Corrige possíveis conflitos de loop assíncrono\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# processa o dataset em lotes pra nao gastar toda ram do colab, salva progresso e evita reprocessar textos\n",
        "async def processar_em_lotes(df, batch_size=100):\n",
        "    resultados = []\n",
        "    start = 0\n",
        "    total = len(df)\n",
        "\n",
        "    # Retoma o progresso se um arquivo parcial já existir\n",
        "    if os.path.exists(output_path):\n",
        "        prev = pd.read_csv(output_path)\n",
        "        start = len(prev)\n",
        "        resultados = prev[\"predito\"].tolist()\n",
        "        print(f\"Retomando {start}/{total}\")\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for i in range(start, total, batch_size):\n",
        "            fim = min(i + batch_size, total)\n",
        "            batch = df.iloc[i:fim][\"texto\"].tolist()\n",
        "            print(f\"Processando  {i+1}-{fim} de {total}\")\n",
        "\n",
        "            # Cria tarefas p cada texto e coleta respostas\n",
        "            tasks = [classificar(session, t) for t in batch]\n",
        "            batch_preds = await asyncio.gather(*tasks)\n",
        "            resultados.extend(batch_preds)\n",
        "\n",
        "            # Salva o progresso a cada lote processado\n",
        "            pd.DataFrame({\n",
        "                \"texto\": df.iloc[:fim][\"texto\"],\n",
        "                \"label\": df.iloc[:fim][\"label\"],\n",
        "                \"predito\": resultados\n",
        "            }).to_csv(output_path, index=False)\n",
        "            print(f\"salvo: {fim}/{total}\")\n",
        "\n",
        "            time.sleep(2)  # pausa p evitar sobrecarga API\n",
        "    return resultados\n",
        "\n",
        "# processamento completo do dataset\n",
        "preds = asyncio.run(processar_em_lotes(df))\n",
        "\n",
        "# Remove previsões inválidas e calcula métricas\n",
        "validos = [i for i in range(len(preds)) if not np.isnan(preds[i])]\n",
        "y_true = df.iloc[validos][\"label\"].values\n",
        "preds_validos = np.array(preds)[validos]\n",
        "\n",
        "print(\"\\nAcurácia:\", round(accuracy_score(y_true, preds_validos), 4))\n",
        "print(\"\\nClassificação:\")\n",
        "print(classification_report(y_true, preds_validos, target_names=[\"FAKE\", \"REAL\"]))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, preds_validos)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Purples\",\n",
        "            xticklabels=[\"Falso Positivo\", \"Verdadeiro Positivo\"],\n",
        "            yticklabels=[\"Verdadeiro Negativo\", \"Falso Negativo\"])\n",
        "plt.title(\"Matriz de Confusão - LLM (GPT-4o-mini)\")\n",
        "plt.xlabel(\"Previsão da LLM\")\n",
        "plt.ylabel(\"Valor Real\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nDuJzA66bPkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}